{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <center> Analysis of the Haunted Places Dataset <center> </h1>\n",
    "\n",
    "<b> Team 3 </b> <br>\n",
    "Members: \n",
    "\n",
    "- Zili Yang\n",
    "- Chen Yi Weng\n",
    "- Aadarsh Sudhir Ghiya \n",
    "- Colin Leahey\n",
    "- Niromikha Jayakumar \n",
    "- Yung Yee Chia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook explores the Haunted Places dataset, extracts new features, joins additional datasets, and analyzes the data using similarity metrics and clustering techniques. The tasks include:\n",
    "1. Preprocessing the dataset.\n",
    "2. Extracting new features from descriptions.\n",
    "3. Joining external datasets.\n",
    "4. Analyzing the combined dataset using Tika-Similarity.\n",
    "5. Visualizing the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download and install Apache Tika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tika in /Users/niromikha/opt/anaconda3/lib/python3.8/site-packages (2.6.0)\n",
      "Requirement already satisfied: requests in /Users/niromikha/opt/anaconda3/lib/python3.8/site-packages (from tika) (2.24.0)\n",
      "Requirement already satisfied: setuptools in /Users/niromikha/opt/anaconda3/lib/python3.8/site-packages (from tika) (50.3.1.post20201107)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/niromikha/opt/anaconda3/lib/python3.8/site-packages (from requests->tika) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/niromikha/opt/anaconda3/lib/python3.8/site-packages (from requests->tika) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/niromikha/opt/anaconda3/lib/python3.8/site-packages (from requests->tika) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/niromikha/opt/anaconda3/lib/python3.8/site-packages (from requests->tika) (1.25.11)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: number-parser in /Users/niromikha/opt/anaconda3/lib/python3.8/site-packages (0.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/niromikha/opt/anaconda3/lib/python3.8/site-packages (from number-parser) (20.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install number-parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datefinder in /Users/niromikha/opt/anaconda3/lib/python3.8/site-packages (0.7.3)\n",
      "Requirement already satisfied: pytz in /Users/niromikha/opt/anaconda3/lib/python3.8/site-packages (from datefinder) (2020.1)\n",
      "Requirement already satisfied: regex>=2017.02.08 in /Users/niromikha/opt/anaconda3/lib/python3.8/site-packages (from datefinder) (2023.5.5)\n",
      "Requirement already satisfied: python-dateutil>=2.4.2 in /Users/niromikha/opt/anaconda3/lib/python3.8/site-packages (from datefinder) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/niromikha/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.4.2->datefinder) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datefinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download the Haunted Places dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# # Load the dataset\n",
    "# haunted_places_df = pd.read_csv('../Data/haunted_places.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a combined TSV file for your Haunted Places dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the original CSV file to TSV\n",
    "# Load the TSV file for further processing\n",
    "# haunted_places_df.to_csv(\"haunted_places.tsv\", sep='\\t', index=False)\n",
    "# haunted_places = pd.read_csv(\"haunted_places.tsv\", sep='\\t')\n",
    "\n",
    "\n",
    "# tsv_path = '../Data/cleaned_haunted_places.tsv'\n",
    "df = pd.read_csv(\"../Data/cleaned_haunted_places.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. a) Audio Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def has_audio_evidence(description):\n",
    "    audio_keywords = [\"noises\", \"sound of snapping neck\", \"nursery rhymes\"]\n",
    "    return any(re.search(rf'\\b{keyword}\\b', description, re.IGNORECASE) for keyword in audio_keywords)\n",
    "\n",
    "df['Audio Evidence'] = df['description'].apply(has_audio_evidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. b) Image/Video/Visual Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_visual_evidence(description):\n",
    "    visual_keywords = [\"cameras\", \"take pictures\", \"names of children written on walls\"]\n",
    "    return any(re.search(rf'\\b{keyword}\\b', description, re.IGNORECASE) for keyword in visual_keywords)\n",
    "\n",
    "df['Image/Video/Visual Evidence'] = df['description'].apply(has_visual_evidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. c) Haunted Places Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datefinder\n",
    "import datetime\n",
    "from datetime import date\n",
    "\n",
    "def extract_date(description):\n",
    "    try:\n",
    "        # Attempt to find dates in the description\n",
    "        matches = datefinder.find_dates(description)\n",
    "        \n",
    "        # Extract the first valid date\n",
    "        for match in matches:\n",
    "            return match.date()  # Return only the date part\n",
    "        \n",
    "    except Exception:\n",
    "        # Silently handle errors without printing messages\n",
    "        pass\n",
    "    \n",
    "    # Fallback to '2025-01-01' if no valid date is found or an error occurs\n",
    "    return datetime.date(2025, 1, 1)\n",
    "\n",
    "# Apply the function to the 'description' column\n",
    "df['Haunted Places Date'] = df['description'].apply(extract_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2025-03-03\n",
       "1        2025-03-01\n",
       "2        2025-01-01\n",
       "3        0211-03-03\n",
       "4        2025-01-01\n",
       "            ...    \n",
       "10969    2025-03-12\n",
       "10970    2025-01-01\n",
       "10971    2025-03-18\n",
       "10972    2025-01-01\n",
       "10973    2025-01-01\n",
       "Name: Haunted Places Date, Length: 10974, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Haunted Places Date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. d) Haunted Places Witness Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             description  \\\n",
      "0      Ada witch - Sometimes you can see a misty blue...   \n",
      "1      A little girl was killed suddenly while waitin...   \n",
      "2      If you take Gorman Rd. west towards Sand Creek...   \n",
      "3      In the 1970's, one room, room 211, in the old ...   \n",
      "4      Kappa Delta Sorority - The Kappa Delta Sororit...   \n",
      "...                                                  ...   \n",
      "10969  at 12 midnight you can see a lady with two lit...   \n",
      "10970  Is haunted by the victims of a murder that hap...   \n",
      "10971  The institution was for kids 18 years old and ...   \n",
      "10972  Gymnasium -  their have been reports of a litt...   \n",
      "10973  Cadets from the Air Force Academy participatin...   \n",
      "\n",
      "       Haunted Places Witness Count  \n",
      "0                                 0  \n",
      "1                                 0  \n",
      "2                                 0  \n",
      "3                                 2  \n",
      "4                                 0  \n",
      "...                             ...  \n",
      "10969                             2  \n",
      "10970                             0  \n",
      "10971                             0  \n",
      "10972                             0  \n",
      "10973                             0  \n",
      "\n",
      "[10974 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from number_parser import parse_number\n",
    "\n",
    "def preprocess_description(description):\n",
    "    \"\"\"Replace vague phrases with estimated numbers to aid extraction.\"\"\"\n",
    "    description = description.lower()\n",
    "    # Replace ambiguous phrases with approximate numbers\n",
    "    replacements = {\n",
    "        \"some\": \"3\",\n",
    "        \"a few\": \"3\",\n",
    "        \"several\": \"5\",\n",
    "        \"many\": \"10\",\n",
    "        \"a lot\": \"10\",\n",
    "        \"a handful\": \"5\",\n",
    "        \"numerous\": \"10\",\n",
    "        \"countless\": \"15\",\n",
    "        \"dozens\": \"12\",\n",
    "        \"scores\": \"20\",\n",
    "        \"hundreds\": \"100\",\n",
    "        \"a couple\": \"2\"\n",
    "    }\n",
    "    \n",
    "    for word, num in replacements.items():\n",
    "        description = re.sub(rf\"\\b{word}\\b\", num, description)  # Whole-word replacement\n",
    "    return description\n",
    "\n",
    "\n",
    "def extract_numbers_from_text(text):\n",
    "    \"\"\"Extract numerical values from written-out numbers in the text.\"\"\"\n",
    "    # Define a regex pattern to match written-out numbers\n",
    "    number_words = r'\\b(?:zero|one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|twenty|thirty|forty|fifty|sixty|seventy|eighty|ninety|hundred|thousand|million|billion)\\b'\n",
    "    \n",
    "    # Find all matches of written-out numbers\n",
    "    matches = re.findall(number_words, text.lower())\n",
    "    \n",
    "    # Parse each match into a numerical value\n",
    "    numbers = [parse_number(match) for match in matches if parse_number(match) is not None]\n",
    "    \n",
    "    # Filter out irrelevant numbers (e.g., years, small numbers)\n",
    "    filtered_numbers = [num for num in numbers if not (1900 <= num <= 2100)]  # Remove years\n",
    "    filtered_numbers = [num for num in filtered_numbers if num > 1]  # Ignore small numbers\n",
    "    \n",
    "    return filtered_numbers\n",
    "\n",
    "\n",
    "def extract_witness_count(description):\n",
    "    \"\"\"\n",
    "    Extract witness count from a haunted place description.\n",
    "    Returns a tuple (witness_count, method) where method indicates how the count was derived.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Preprocess the description\n",
    "        preprocessed_text = preprocess_description(description)\n",
    "        \n",
    "        # Step 2: Extract numbers from the text\n",
    "        numbers = extract_numbers_from_text(preprocessed_text)\n",
    "        \n",
    "        # Step 3: Return the first valid number found, or 0 if no numbers are found\n",
    "        if numbers:\n",
    "            return numbers[0], \"explicit_number\"\n",
    "        \n",
    "        # Step 4: Default to 0 if no numbers are found\n",
    "        return 0, \"default\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing witness count from description: {description[:100]}... Error: {e}\")\n",
    "        return 0, \"error\"\n",
    "\n",
    "\n",
    "# Apply the function to the 'description' column\n",
    "df['Haunted Places Witness Count'] = df['description'].apply(\n",
    "    lambda desc: extract_witness_count(desc)[0]  # Extract only the count (not the method)\n",
    ")\n",
    "\n",
    "# Display the updated columns\n",
    "print(df[['description', 'Haunted Places Witness Count']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. e) Time of Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_of_day(description):\n",
    "    time_keywords = {\"evening\": \"Evening\", \"morning\": \"Morning\", \"dusk\": \"Dusk\"}\n",
    "    for keyword, time_of_day in time_keywords.items():\n",
    "        if re.search(rf'\\b{keyword}\\b', description, re.IGNORECASE):\n",
    "            return time_of_day\n",
    "    return \"Unknown\"\n",
    "\n",
    "df['Time of Day'] = df['description'].apply(extract_time_of_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. f) Apparition Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_apparition_type(description):\n",
    "    apparition_keywords = {\n",
    "        \"ghost\": \"Ghost\",\n",
    "        \"orb\": \"Orb\",\n",
    "        \"ufo\": \"UFO\",\n",
    "        \"uap\": \"UAP\",\n",
    "        \"male\": \"Male\",\n",
    "        \"female\": \"Female\",\n",
    "        \"child\": \"Child\",\n",
    "        \"several ghosts\": \"Several Ghosts\"\n",
    "    }\n",
    "    for keyword, apparition_type in apparition_keywords.items():\n",
    "        if re.search(rf'\\b{keyword}\\b', description, re.IGNORECASE):\n",
    "            return apparition_type\n",
    "    return \"Unknown\"\n",
    "\n",
    "df['Apparition Type'] = df['description'].apply(extract_apparition_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. g) Event type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_event_type(description):\n",
    "    event_keywords = {\n",
    "        \"murder\": \"Murder\",\n",
    "        \"die\": \"Death\",\n",
    "        \"supernatural\": \"Supernatural Phenomenon\"\n",
    "    }\n",
    "    for keyword, event_type in event_keywords.items():\n",
    "        if re.search(rf'\\b{keyword}\\b', description, re.IGNORECASE):\n",
    "            return event_type\n",
    "    return \"Unknown\"\n",
    "\n",
    "df['Event Type'] = df['description'].apply(extract_event_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          city        country  \\\n",
      "1518     Lenox  United States   \n",
      "3390  Woodward  United States   \n",
      "8509     Paris  United States   \n",
      "\n",
      "                                            description  \\\n",
      "1518  An eleven year old girl, whose last name is Sl...   \n",
      "3390  This coffee house was originally a doctor's of...   \n",
      "8509  Most people get a bad feeling just looking at ...   \n",
      "\n",
      "                                 location          state state_abbrev  \\\n",
      "1518                     Cranewell Resort  Massachusetts           MA   \n",
      "3390                     Leos Coffeehouse       Oklahoma           OK   \n",
      "8509  Old Plantation home in Slate Shoals          Texas           TX   \n",
      "\n",
      "      longitude   latitude  city_longitude  city_latitude  Audio Evidence  \\\n",
      "1518 -73.267236  42.341822      -73.284876      42.356461           False   \n",
      "3390 -99.393019  36.434108      -99.390386      36.433648           False   \n",
      "8509 -95.555513  33.660939      -95.555513      33.660939           False   \n",
      "\n",
      "      Image/Video/Visual Evidence Haunted Places Date  \\\n",
      "1518                        False          2025-01-01   \n",
      "3390                        False          2025-01-01   \n",
      "8509                        False          2025-05-03   \n",
      "\n",
      "      Haunted Places Witness Count Time of Day Apparition Type Event Type  \n",
      "1518                            11     Unknown         Unknown    Unknown  \n",
      "3390                            11     Unknown         Unknown    Unknown  \n",
      "8509                            11     Unknown         Unknown    Unknown  \n"
     ]
    }
   ],
   "source": [
    "#to check\n",
    "visual_evidence_records = df[df['Haunted Places Witness Count'] == 11]\n",
    "\n",
    "# Display the filtered records\n",
    "print(visual_evidence_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. h) Merge the Alcohol Abuse Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge completed: 10974 rows merged.\n"
     ]
    }
   ],
   "source": [
    "#  Haunted Places dataset (TSV format)\n",
    "df.columns = [col.strip().lower() for col in df.columns]\n",
    "\n",
    "# Load Alcohol Abuse dataset (appears to be comma-separated)\n",
    "alcohol_df = pd.read_csv(\"../Data/alcohol_abuse.tsv\", sep=\",\")\n",
    "alcohol_df.columns = [col.strip().lower() for col in alcohol_df.columns]\n",
    "\n",
    "# Rename the Alcohol Abuse column to ensure it has a common key (\"state\")\n",
    "if \"state\" not in alcohol_df.columns and \"state_name\" in alcohol_df.columns:\n",
    "    alcohol_df.rename(columns={\"state_name\": \"state\"}, inplace=True)\n",
    "\n",
    "# Check that both DataFrames have the 'state' column\n",
    "if \"state\" not in df.columns or \"state\" not in alcohol_df.columns:\n",
    "    raise KeyError(\"The 'state' column is missing from one of the datasets.\")\n",
    "\n",
    "# Merge the datasets on the 'state' column using a left join\n",
    "merged_df = pd.merge(df, alcohol_df, on=\"state\", how=\"left\")\n",
    "\n",
    "# Save the merged dataset as a TSV file\n",
    "merged_df.to_csv(\"../Data/haunted_places_with_alcohol.tsv\", sep=\"\\t\", index=False)\n",
    "print(\"Merge completed: {} rows merged.\".format(merged_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. i) Merge the Alcohol Abuse Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.27.1-py3-none-any.whl (9.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.7 MB 13.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Collecting urllib3[socks]<3,>=1.26\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Collecting typing_extensions~=4.9\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Collecting websocket-client~=1.8\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 3.1 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting trio~=0.17\n",
      "  Downloading trio-0.27.0-py3-none-any.whl (481 kB)\n",
      "\u001b[K     |████████████████████████████████| 481 kB 8.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2021.10.8\n",
      "  Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "\u001b[K     |████████████████████████████████| 166 kB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting outcome>=1.2.0\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Collecting exceptiongroup; python_version < \"3.11\"\n",
      "  Using cached exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6; extra == \"socks\" in /Users/niromikha/opt/anaconda3/lib/python3.8/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /Users/niromikha/opt/anaconda3/lib/python3.8/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: idna in /Users/niromikha/opt/anaconda3/lib/python3.8/site-packages (from trio~=0.17->selenium) (2.10)\n",
      "Requirement already satisfied: sortedcontainers in /Users/niromikha/opt/anaconda3/lib/python3.8/site-packages (from trio~=0.17->selenium) (2.2.2)\n",
      "Collecting attrs>=23.2.0\n",
      "  Downloading attrs-25.1.0-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 553 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h11<1,>=0.9.0 in /Users/niromikha/opt/anaconda3/lib/python3.8/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Installing collected packages: attrs, outcome, wsproto, exceptiongroup, trio, trio-websocket, urllib3, typing-extensions, websocket-client, certifi, selenium\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 20.3.0\n",
      "    Uninstalling attrs-20.3.0:\n",
      "      Successfully uninstalled attrs-20.3.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.11\n",
      "    Uninstalling urllib3-1.25.11:\n",
      "      Successfully uninstalled urllib3-1.25.11\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.7.4.3\n",
      "    Uninstalling typing-extensions-3.7.4.3:\n",
      "      Successfully uninstalled typing-extensions-3.7.4.3\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2020.6.20\n",
      "    Uninstalling certifi-2020.6.20:\n",
      "      Successfully uninstalled certifi-2020.6.20\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "conda 4.13.0 requires ruamel_yaml_conda>=0.11.14, which is not installed.\n",
      "requests 2.24.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 2.2.3 which is incompatible.\u001b[0m\n",
      "Successfully installed attrs-25.1.0 certifi-2025.1.31 exceptiongroup-1.2.2 outcome-1.3.0.post0 selenium-4.27.1 trio-0.27.0 trio-websocket-0.12.2 typing-extensions-4.12.2 urllib3-2.2.3 websocket-client-1.8.0 wsproto-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tables found: 3\n",
      "Full daylight data saved to daylight_hours_full.tsv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Set up Selenium to run headlessly (no browser window)\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "driver = webdriver.Chrome(options=options)  # Ensure chromedriver is in your PATH\n",
    "\n",
    "# URL for the daylight data page (modify if needed)\n",
    "url = \"https://www.timeanddate.com/astronomy/usa\"\n",
    "driver.get(url)\n",
    "driver.implicitly_wait(10)  # Wait for dynamic content to load\n",
    "\n",
    "# Get the full page source after dynamic content has loaded\n",
    "html = driver.page_source\n",
    "driver.quit()\n",
    "\n",
    "# Parse the HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Find all table elements on the page\n",
    "tables = soup.find_all(\"table\")\n",
    "print(\"Number of tables found:\", len(tables))\n",
    "\n",
    "# Extract each table into a DataFrame and combine them\n",
    "dfs = [pd.read_html(str(table))[0] for table in tables]\n",
    "if len(dfs) > 1:\n",
    "    full_df = pd.concat(dfs, ignore_index=True)\n",
    "else:\n",
    "    full_df = dfs[0]\n",
    "\n",
    "# Save the full dataset as a TSV file\n",
    "full_df.to_csv(\"../Data/daylight_hours_full.tsv\", sep=\"\\t\", index=False)\n",
    "print(\"Full daylight data saved to daylight_hours_full.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daylight Hours columns: ['0', '1', '↑ sunrise and ↓ sunset in united states (79 locations)', '↑ sunrise and ↓ sunset in united states (79 locations).1', '↑ sunrise and ↓ sunset in united states (79 locations).2', '↑ sunrise and ↓ sunset in united states (79 locations).3', '↑ sunrise and ↓ sunset in united states (79 locations).4', '↑ sunrise and ↓ sunset in united states (79 locations).5', '↑ sunrise and ↓ sunset in united states (79 locations).6', '↑ sunrise and ↓ sunset in united states (79 locations).7', '↑ sunrise and ↓ sunset in united states (79 locations).8', 'find sunrise and sunset for other places…', 'find sunrise and sunset for other places….1', 'find sunrise and sunset for other places….2', 'find sunrise and sunset for other places….3', 'find sunrise and sunset for other places….4']\n",
      "0          Country:\n",
      "1        Long Name:\n",
      "2    Abbreviations:\n",
      "3          Capital:\n",
      "4       Time Zones:\n",
      "Name: 0, dtype: object\n",
      "                   0 state\n",
      "0           Country:  None\n",
      "1         Long Name:  None\n",
      "2     Abbreviations:  None\n",
      "3           Capital:  None\n",
      "4        Time Zones:  None\n",
      "5  Total Time Zones:  None\n",
      "6         Dial Code:  None\n",
      "7                NaN  None\n",
      "8                NaN  None\n",
      "9                NaN  None\n",
      "Merge completed: 10974 rows merged.\n"
     ]
    }
   ],
   "source": [
    "# Load the merged Haunted Places with Alcohol dataset (TSV)\n",
    "merged_alcohol_df = pd.read_csv(\"../Data/haunted_places_with_alcohol.tsv\", sep=\"\\t\")\n",
    "merged_alcohol_df.columns = [col.strip().lower() for col in merged_alcohol_df.columns]\n",
    "\n",
    "# Load the Daylight Hours dataset (TSV)\n",
    "daylight_df = pd.read_csv(\"../Data/daylight_hours_full.tsv\", sep=\"\\t\")\n",
    "daylight_df.columns = [col.strip().lower() for col in daylight_df.columns]\n",
    "\n",
    "# Inspect the daylight dataset columns and preview column \"0\"\n",
    "print(\"Daylight Hours columns:\", daylight_df.columns.tolist())\n",
    "print(daylight_df[\"0\"].head(5))\n",
    "\n",
    "# Define a function to extract a two-letter state abbreviation from a string.\n",
    "# This function assumes the format \"City Name (XX)\" where XX is the state abbreviation.\n",
    "def extract_state(location):\n",
    "    match = re.search(r'\\(([A-Z]{2})\\)', location)\n",
    "    if match:\n",
    "        return match.group(1).lower()  # return in lowercase for consistency\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create a new 'state' column by extracting from column \"0\"\n",
    "daylight_df[\"state\"] = daylight_df[\"0\"].apply(lambda x: extract_state(str(x)))\n",
    "print(daylight_df[[\"0\", \"state\"]].head(10))\n",
    "\n",
    "# Check that both DataFrames have the 'state' column\n",
    "if \"state\" not in merged_alcohol_df.columns:\n",
    "    raise KeyError(\"The 'state' column is missing from the Haunted Places dataset.\")\n",
    "if \"state\" not in daylight_df.columns:\n",
    "    raise KeyError(\"The 'state' column is missing from the Daylight Hours dataset.\")\n",
    "\n",
    "# Merge the datasets on the 'state' column using a left join\n",
    "merged_final_df = pd.merge(merged_alcohol_df, daylight_df, on=\"state\", how=\"left\")\n",
    "\n",
    "# Save the final merged dataset as a TSV file\n",
    "merged_final_df.to_csv(\"../Data/haunted_places_with_alcohol_daylight.tsv\", sep=\"\\t\", index=False)\n",
    "print(\"Merge completed: {} rows merged.\".format(merged_final_df.shape[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
