{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download and install necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tika\n",
      "  Downloading tika-2.6.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from tika) (69.5.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from tika) (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->tika) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->tika) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->tika) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->tika) (2024.8.30)\n",
      "Building wheels for collected packages: tika\n",
      "  Building wheel for tika (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tika: filename=tika-2.6.0-py3-none-any.whl size=32621 sha256=a29acf9aaaf45f56aa8d739bf04b433f96e5e9028f94237167009ee824defa0e\n",
      "  Stored in directory: /Users/frankcisyang/Library/Caches/pip/wheels/ad/75/cc/cb91a96aab7a28cac9a04967c6034162d50dd441c1709aeea7\n",
      "Successfully built tika\n",
      "Installing collected packages: tika\n",
      "Successfully installed tika-2.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting number-parser\n",
      "  Downloading number_parser-0.3.2-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from number-parser) (23.1.0)\n",
      "Downloading number_parser-0.3.2-py2.py3-none-any.whl (51 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: number-parser\n",
      "Successfully installed number-parser-0.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install number-parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datefinder\n",
      "  Downloading datefinder-0.7.3-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: regex>=2017.02.08 in /opt/anaconda3/lib/python3.12/site-packages (from datefinder) (2023.10.3)\n",
      "Requirement already satisfied: python-dateutil>=2.4.2 in /opt/anaconda3/lib/python3.12/site-packages (from datefinder) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz in /opt/anaconda3/lib/python3.12/site-packages (from datefinder) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.4.2->datefinder) (1.16.0)\n",
      "Downloading datefinder-0.7.3-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: datefinder\n",
      "Successfully installed datefinder-0.7.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datefinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the cleaned Haunted Places dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../Data/cleaned_haunted_places.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract features from the \"description\" field\n",
    "### 3. a) Audio Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def has_audio_evidence(description):\n",
    "    audio_keywords = [\"noises\", \"sound of snapping neck\", \"nursery rhymes\"]\n",
    "    return any(re.search(rf'\\b{keyword}\\b', description, re.IGNORECASE) for keyword in audio_keywords)\n",
    "\n",
    "df['Audio Evidence'] = df['description'].apply(has_audio_evidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. b) Image/Video/Visual Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_visual_evidence(description):\n",
    "    visual_keywords = [\"cameras\", \"take pictures\", \"names of children written on walls\"]\n",
    "    return any(re.search(rf'\\b{keyword}\\b', description, re.IGNORECASE) for keyword in visual_keywords)\n",
    "\n",
    "df['Image/Video/Visual Evidence'] = df['description'].apply(has_visual_evidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. c) Haunted Places Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datefinder\n",
    "import datetime\n",
    "from datetime import date\n",
    "\n",
    "def extract_date(description):\n",
    "    try:\n",
    "        # Attempt to find dates in the description\n",
    "        matches = datefinder.find_dates(description)\n",
    "        \n",
    "        # Extract the first valid date\n",
    "        for match in matches:\n",
    "            return match.date()  # Return only the date part\n",
    "        \n",
    "    except Exception:\n",
    "        # Silently handle errors without printing messages\n",
    "        pass\n",
    "    \n",
    "    # Fallback to '2025-01-01' if no valid date is found or an error occurs\n",
    "    return datetime.date(2025, 1, 1)\n",
    "\n",
    "# Apply the function to the 'description' column\n",
    "df['Haunted Places Date'] = df['description'].apply(extract_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2025-03-03\n",
       "1        2025-03-01\n",
       "2        2025-01-01\n",
       "3        0211-03-04\n",
       "4        2025-01-01\n",
       "            ...    \n",
       "10969    2025-03-12\n",
       "10970    2025-01-01\n",
       "10971    2025-03-18\n",
       "10972    2025-01-01\n",
       "10973    2025-01-01\n",
       "Name: Haunted Places Date, Length: 10974, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Haunted Places Date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. d) Haunted Places Witness Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             description  \\\n",
      "0      Ada witch - Sometimes you can see a misty blue...   \n",
      "1      A little girl was killed suddenly while waitin...   \n",
      "2      If you take Gorman Rd. west towards Sand Creek...   \n",
      "3      In the 1970's, one room, room 211, in the old ...   \n",
      "4      Kappa Delta Sorority - The Kappa Delta Sororit...   \n",
      "...                                                  ...   \n",
      "10969  at 12 midnight you can see a lady with two lit...   \n",
      "10970  Is haunted by the victims of a murder that hap...   \n",
      "10971  The institution was for kids 18 years old and ...   \n",
      "10972  Gymnasium -  their have been reports of a litt...   \n",
      "10973  Cadets from the Air Force Academy participatin...   \n",
      "\n",
      "       Haunted Places Witness Count  \n",
      "0                                 0  \n",
      "1                                 0  \n",
      "2                                 0  \n",
      "3                                 2  \n",
      "4                                 0  \n",
      "...                             ...  \n",
      "10969                             2  \n",
      "10970                             0  \n",
      "10971                             0  \n",
      "10972                             0  \n",
      "10973                             0  \n",
      "\n",
      "[10974 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from number_parser import parse_number\n",
    "\n",
    "def preprocess_description(description):\n",
    "    \"\"\"Replace vague phrases with estimated numbers to aid extraction.\"\"\"\n",
    "    description = description.lower()\n",
    "    # Replace ambiguous phrases with approximate numbers\n",
    "    replacements = {\n",
    "        \"some\": \"3\",\n",
    "        \"a few\": \"3\",\n",
    "        \"several\": \"5\",\n",
    "        \"many\": \"10\",\n",
    "        \"a lot\": \"10\",\n",
    "        \"a handful\": \"5\",\n",
    "        \"numerous\": \"10\",\n",
    "        \"countless\": \"15\",\n",
    "        \"dozens\": \"12\",\n",
    "        \"scores\": \"20\",\n",
    "        \"hundreds\": \"100\",\n",
    "        \"a couple\": \"2\"\n",
    "    }\n",
    "    \n",
    "    for word, num in replacements.items():\n",
    "        description = re.sub(rf\"\\b{word}\\b\", num, description)  # Whole-word replacement\n",
    "    return description\n",
    "\n",
    "\n",
    "def extract_numbers_from_text(text):\n",
    "    \"\"\"Extract numerical values from written-out numbers in the text.\"\"\"\n",
    "    # Define a regex pattern to match written-out numbers\n",
    "    number_words = r'\\b(?:zero|one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|twenty|thirty|forty|fifty|sixty|seventy|eighty|ninety|hundred|thousand|million|billion)\\b'\n",
    "    \n",
    "    # Find all matches of written-out numbers\n",
    "    matches = re.findall(number_words, text.lower())\n",
    "    \n",
    "    # Parse each match into a numerical value\n",
    "    numbers = [parse_number(match) for match in matches if parse_number(match) is not None]\n",
    "    \n",
    "    # Filter out irrelevant numbers (e.g., years, small numbers)\n",
    "    filtered_numbers = [num for num in numbers if not (1900 <= num <= 2100)]  # Remove years\n",
    "    filtered_numbers = [num for num in filtered_numbers if num > 1]  # Ignore small numbers\n",
    "    \n",
    "    return filtered_numbers\n",
    "\n",
    "\n",
    "def extract_witness_count(description):\n",
    "    \"\"\"\n",
    "    Extract witness count from a haunted place description.\n",
    "    Returns a tuple (witness_count, method) where method indicates how the count was derived.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Preprocess the description\n",
    "        preprocessed_text = preprocess_description(description)\n",
    "        \n",
    "        # Step 2: Extract numbers from the text\n",
    "        numbers = extract_numbers_from_text(preprocessed_text)\n",
    "        \n",
    "        # Step 3: Return the first valid number found, or 0 if no numbers are found\n",
    "        if numbers:\n",
    "            return numbers[0], \"explicit_number\"\n",
    "        \n",
    "        # Step 4: Default to 0 if no numbers are found\n",
    "        return 0, \"default\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing witness count from description: {description[:100]}... Error: {e}\")\n",
    "        return 0, \"error\"\n",
    "\n",
    "\n",
    "# Apply the function to the 'description' column\n",
    "df['Haunted Places Witness Count'] = df['description'].apply(\n",
    "    lambda desc: extract_witness_count(desc)[0]  # Extract only the count (not the method)\n",
    ")\n",
    "\n",
    "# Display the updated columns\n",
    "print(df[['description', 'Haunted Places Witness Count']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. e) Time of Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_of_day(description):\n",
    "    time_keywords = {\"evening\": \"Evening\", \"morning\": \"Morning\", \"dusk\": \"Dusk\"}\n",
    "    for keyword, time_of_day in time_keywords.items():\n",
    "        if re.search(rf'\\b{keyword}\\b', description, re.IGNORECASE):\n",
    "            return time_of_day\n",
    "    return \"Unknown\"\n",
    "\n",
    "df['Time of Day'] = df['description'].apply(extract_time_of_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. f) Apparition Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_apparition_type(description):\n",
    "    apparition_keywords = {\n",
    "        \"ghost\": \"Ghost\",\n",
    "        \"orb\": \"Orb\",\n",
    "        \"ufo\": \"UFO\",\n",
    "        \"uap\": \"UAP\",\n",
    "        \"male\": \"Male\",\n",
    "        \"female\": \"Female\",\n",
    "        \"child\": \"Child\",\n",
    "        \"several ghosts\": \"Several Ghosts\"\n",
    "    }\n",
    "    for keyword, apparition_type in apparition_keywords.items():\n",
    "        if re.search(rf'\\b{keyword}\\b', description, re.IGNORECASE):\n",
    "            return apparition_type\n",
    "    return \"Unknown\"\n",
    "\n",
    "df['Apparition Type'] = df['description'].apply(extract_apparition_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. g) Event type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_event_type(description):\n",
    "    event_keywords = {\n",
    "        \"murder\": \"Murder\",\n",
    "        \"die\": \"Death\",\n",
    "        \"supernatural\": \"Supernatural Phenomenon\"\n",
    "    }\n",
    "    for keyword, event_type in event_keywords.items():\n",
    "        if re.search(rf'\\b{keyword}\\b', description, re.IGNORECASE):\n",
    "            return event_type\n",
    "    return \"Unknown\"\n",
    "\n",
    "df['Event Type'] = df['description'].apply(extract_event_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          city        country  \\\n",
      "1518     Lenox  United States   \n",
      "3390  Woodward  United States   \n",
      "8509     Paris  United States   \n",
      "\n",
      "                                            description  \\\n",
      "1518  An eleven year old girl, whose last name is Sl...   \n",
      "3390  This coffee house was originally a doctor's of...   \n",
      "8509  Most people get a bad feeling just looking at ...   \n",
      "\n",
      "                                 location          state state_abbrev  \\\n",
      "1518                     Cranewell Resort  Massachusetts           MA   \n",
      "3390                     Leos Coffeehouse       Oklahoma           OK   \n",
      "8509  Old Plantation home in Slate Shoals          Texas           TX   \n",
      "\n",
      "      longitude   latitude  city_longitude  city_latitude  Audio Evidence  \\\n",
      "1518 -73.267236  42.341822      -73.284876      42.356461           False   \n",
      "3390 -99.393019  36.434108      -99.390386      36.433648           False   \n",
      "8509 -95.555513  33.660939      -95.555513      33.660939           False   \n",
      "\n",
      "      Image/Video/Visual Evidence Haunted Places Date  \\\n",
      "1518                        False          2025-01-01   \n",
      "3390                        False          2025-01-01   \n",
      "8509                        False          2025-05-04   \n",
      "\n",
      "      Haunted Places Witness Count Time of Day Apparition Type Event Type  \n",
      "1518                            11     Unknown         Unknown    Unknown  \n",
      "3390                            11     Unknown         Unknown    Unknown  \n",
      "8509                            11     Unknown         Unknown    Unknown  \n"
     ]
    }
   ],
   "source": [
    "#to check\n",
    "visual_evidence_records = df[df['Haunted Places Witness Count'] == 11]\n",
    "\n",
    "# Display the filtered records\n",
    "print(visual_evidence_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. h) Merge the Alcohol Abuse Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge completed: 10974 rows merged.\n"
     ]
    }
   ],
   "source": [
    "#  Haunted Places dataset (TSV format)\n",
    "df.columns = [col.strip().lower() for col in df.columns]\n",
    "\n",
    "# Load Alcohol Abuse dataset (appears to be comma-separated)\n",
    "alcohol_df = pd.read_csv(\"../Data/alcohol_abuse.tsv\", sep=\",\")\n",
    "alcohol_df.columns = [col.strip().lower() for col in alcohol_df.columns]\n",
    "\n",
    "# Rename the Alcohol Abuse column to ensure it has a common key (\"state\")\n",
    "if \"state\" not in alcohol_df.columns and \"state_name\" in alcohol_df.columns:\n",
    "    alcohol_df.rename(columns={\"state_name\": \"state\"}, inplace=True)\n",
    "\n",
    "# Check that both DataFrames have the 'state' column\n",
    "if \"state\" not in df.columns or \"state\" not in alcohol_df.columns:\n",
    "    raise KeyError(\"The 'state' column is missing from one of the datasets.\")\n",
    "\n",
    "# Merge the datasets on the 'state' column using a left join\n",
    "merged_df = pd.merge(df, alcohol_df, on=\"state\", how=\"left\")\n",
    "\n",
    "# Save the merged dataset as a TSV file\n",
    "merged_df.to_csv(\"../Data/haunted_places_with_alcohol.tsv\", sep=\"\\t\", index=False)\n",
    "print(\"Merge completed: {} rows merged.\".format(merged_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. i) Merge the Alcohol Abuse Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.29.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /opt/anaconda3/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.2)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.29.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (2024.8.30)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (1.8.0)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.17->selenium)\n",
      "  Downloading attrs-25.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: sortedcontainers in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Downloading selenium-4.29.0-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading trio-0.29.0-py3-none-any.whl (492 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.9/492.9 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Downloading attrs-25.1.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.2/63.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: wsproto, attrs, outcome, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.1.0\n",
      "    Uninstalling attrs-23.1.0:\n",
      "      Successfully uninstalled attrs-23.1.0\n",
      "Successfully installed attrs-25.1.0 outcome-1.3.0.post0 selenium-4.29.0 trio-0.29.0 trio-websocket-0.12.2 wsproto-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tables found: 3\n",
      "Full daylight data saved to daylight_hours_full.tsv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Set up Selenium to run headlessly (no browser window)\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "driver = webdriver.Chrome(options=options)  # Ensure chromedriver is in your PATH\n",
    "\n",
    "# URL for the daylight data page (modify if needed)\n",
    "url = \"https://www.timeanddate.com/astronomy/usa\"\n",
    "driver.get(url)\n",
    "driver.implicitly_wait(10)  # Wait for dynamic content to load\n",
    "\n",
    "# Get the full page source after dynamic content has loaded\n",
    "html = driver.page_source\n",
    "driver.quit()\n",
    "\n",
    "# Parse the HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Find all table elements on the page\n",
    "tables = soup.find_all(\"table\")\n",
    "print(\"Number of tables found:\", len(tables))\n",
    "\n",
    "# Extract each table into a DataFrame and combine them\n",
    "dfs = [pd.read_html(str(table))[0] for table in tables]\n",
    "if len(dfs) > 1:\n",
    "    full_df = pd.concat(dfs, ignore_index=True)\n",
    "else:\n",
    "    full_df = dfs[0]\n",
    "\n",
    "# Save the full dataset as a TSV file\n",
    "full_df.to_csv(\"../Data/daylight_hours_full.tsv\", sep=\"\\t\", index=False)\n",
    "print(\"Full daylight data saved to daylight_hours_full.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daylight Hours columns: ['0', '1', '↑ sunrise and ↓ sunset in united states (79 locations)', '↑ sunrise and ↓ sunset in united states (79 locations).1', '↑ sunrise and ↓ sunset in united states (79 locations).2', '↑ sunrise and ↓ sunset in united states (79 locations).3', '↑ sunrise and ↓ sunset in united states (79 locations).4', '↑ sunrise and ↓ sunset in united states (79 locations).5', '↑ sunrise and ↓ sunset in united states (79 locations).6', '↑ sunrise and ↓ sunset in united states (79 locations).7', '↑ sunrise and ↓ sunset in united states (79 locations).8', 'find sunrise and sunset for other places…', 'find sunrise and sunset for other places….1', 'find sunrise and sunset for other places….2', 'find sunrise and sunset for other places….3', 'find sunrise and sunset for other places….4']\n",
      "0          Country:\n",
      "1        Long Name:\n",
      "2    Abbreviations:\n",
      "3          Capital:\n",
      "4       Time Zones:\n",
      "Name: 0, dtype: object\n",
      "                   0 state\n",
      "0           Country:  None\n",
      "1         Long Name:  None\n",
      "2     Abbreviations:  None\n",
      "3           Capital:  None\n",
      "4        Time Zones:  None\n",
      "5  Total Time Zones:  None\n",
      "6         Dial Code:  None\n",
      "7                NaN  None\n",
      "8                NaN  None\n",
      "9                NaN  None\n",
      "Merge completed: 10974 rows merged.\n"
     ]
    }
   ],
   "source": [
    "# Load the merged Haunted Places with Alcohol dataset (TSV)\n",
    "merged_alcohol_df = pd.read_csv(\"../Data/haunted_places_with_alcohol.tsv\", sep=\"\\t\")\n",
    "merged_alcohol_df.columns = [col.strip().lower() for col in merged_alcohol_df.columns]\n",
    "\n",
    "# Load the Daylight Hours dataset (TSV)\n",
    "daylight_df = pd.read_csv(\"../Data/daylight_hours_full.tsv\", sep=\"\\t\")\n",
    "daylight_df.columns = [col.strip().lower() for col in daylight_df.columns]\n",
    "\n",
    "# Inspect the daylight dataset columns and preview column \"0\"\n",
    "print(\"Daylight Hours columns:\", daylight_df.columns.tolist())\n",
    "print(daylight_df[\"0\"].head(5))\n",
    "\n",
    "# Define a function to extract a two-letter state abbreviation from a string.\n",
    "# This function assumes the format \"City Name (XX)\" where XX is the state abbreviation.\n",
    "def extract_state(location):\n",
    "    match = re.search(r'\\(([A-Z]{2})\\)', location)\n",
    "    if match:\n",
    "        return match.group(1).lower()  # return in lowercase for consistency\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create a new 'state' column by extracting from column \"0\"\n",
    "daylight_df[\"state\"] = daylight_df[\"0\"].apply(lambda x: extract_state(str(x)))\n",
    "print(daylight_df[[\"0\", \"state\"]].head(10))\n",
    "\n",
    "# Check that both DataFrames have the 'state' column\n",
    "if \"state\" not in merged_alcohol_df.columns:\n",
    "    raise KeyError(\"The 'state' column is missing from the Haunted Places dataset.\")\n",
    "if \"state\" not in daylight_df.columns:\n",
    "    raise KeyError(\"The 'state' column is missing from the Daylight Hours dataset.\")\n",
    "\n",
    "# Merge the datasets on the 'state' column using a left join\n",
    "merged_final_df = pd.merge(merged_alcohol_df, daylight_df, on=\"state\", how=\"left\")\n",
    "\n",
    "# Save the final merged dataset as a TSV file\n",
    "merged_final_df.to_csv(\"../Data/haunted_places_with_alcohol_daylight.tsv\", sep=\"\\t\", index=False)\n",
    "print(\"Merge completed: {} rows merged.\".format(merged_final_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
