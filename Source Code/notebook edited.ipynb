{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <center> Analysis of the Haunted Places Dataset <center> </h1>\n",
    "\n",
    "<b> Team 3 </b> <br>\n",
    "Members: \n",
    "\n",
    "- Zili Yang\n",
    "- Chen Yi Weng\n",
    "- Aadarsh Sudhir Ghiya \n",
    "- Colin Leahey\n",
    "- Niromikha Jayakumar \n",
    "- Yung Yee Chia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook explores the Haunted Places dataset, extracts new features, joins additional datasets, and analyzes the data using similarity metrics and clustering techniques. The tasks include:\n",
    "1. Preprocessing the dataset.\n",
    "2. Extracting new features from descriptions.\n",
    "3. Joining external datasets.\n",
    "4. Analyzing the combined dataset using Tika-Similarity.\n",
    "5. Visualizing the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download and install Apache Tika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tika in /Users/yungyeechia/miniconda3/lib/python3.11/site-packages (2.6.0)\n",
      "Requirement already satisfied: setuptools in /Users/yungyeechia/miniconda3/lib/python3.11/site-packages (from tika) (67.8.0)\n",
      "Requirement already satisfied: requests in /Users/yungyeechia/miniconda3/lib/python3.11/site-packages (from tika) (2.29.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/yungyeechia/miniconda3/lib/python3.11/site-packages (from requests->tika) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yungyeechia/miniconda3/lib/python3.11/site-packages (from requests->tika) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/yungyeechia/miniconda3/lib/python3.11/site-packages (from requests->tika) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yungyeechia/miniconda3/lib/python3.11/site-packages (from requests->tika) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: number-parser in /Users/yungyeechia/miniconda3/lib/python3.11/site-packages (0.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/yungyeechia/miniconda3/lib/python3.11/site-packages (from number-parser) (25.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install number-parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datefinder in /Users/yungyeechia/miniconda3/lib/python3.11/site-packages (0.7.3)\n",
      "Requirement already satisfied: regex>=2017.02.08 in /Users/yungyeechia/miniconda3/lib/python3.11/site-packages (from datefinder) (2024.11.6)\n",
      "Requirement already satisfied: python-dateutil>=2.4.2 in /Users/yungyeechia/miniconda3/lib/python3.11/site-packages (from datefinder) (2.8.2)\n",
      "Requirement already satisfied: pytz in /Users/yungyeechia/miniconda3/lib/python3.11/site-packages (from datefinder) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/yungyeechia/miniconda3/lib/python3.11/site-packages (from python-dateutil>=2.4.2->datefinder) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datefinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download the Haunted Places dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "haunted_places_df = pd.read_csv('../Data/haunted_places.csv')\n",
    "\n",
    "# Make a copy of the original dataset\n",
    "haunted_places_copy = haunted_places_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a combined TSV file for your Haunted Places dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the original CSV file to TSV\n",
    "haunted_places_df = pd.read_csv(\"../Data/haunted_places.csv\")\n",
    "haunted_places_df.to_csv(\"haunted_places.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TSV file for further processing\n",
    "haunted_places = pd.read_csv(\"haunted_places.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. a) Audio Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def has_audio_evidence(description):\n",
    "    audio_keywords = [\"noises\", \"sound of snapping neck\", \"nursery rhymes\"]\n",
    "    return any(re.search(rf'\\b{keyword}\\b', description, re.IGNORECASE) for keyword in audio_keywords)\n",
    "\n",
    "haunted_places['Audio Evidence'] = haunted_places['description'].apply(has_audio_evidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. b) Image/Video/Visual Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_visual_evidence(description):\n",
    "    visual_keywords = [\"cameras\", \"take pictures\", \"names of children written on walls\"]\n",
    "    return any(re.search(rf'\\b{keyword}\\b', description, re.IGNORECASE) for keyword in visual_keywords)\n",
    "\n",
    "haunted_places['Image/Video/Visual Evidence'] = haunted_places['description'].apply(has_visual_evidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. c) Haunted Places Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datefinder\n",
    "import datetime\n",
    "from datetime import date\n",
    "\n",
    "def extract_date(description):\n",
    "    try:\n",
    "        # Attempt to find dates in the description\n",
    "        matches = datefinder.find_dates(description)\n",
    "        \n",
    "        # Extract the first valid date\n",
    "        for match in matches:\n",
    "            return match.date()  # Return only the date part\n",
    "        \n",
    "    except Exception:\n",
    "        # Silently handle errors without printing messages\n",
    "        pass\n",
    "    \n",
    "    # Fallback to '2025-01-01' if no valid date is found or an error occurs\n",
    "    return datetime.date(2025, 1, 1)\n",
    "\n",
    "# Apply the function to the 'description' column\n",
    "haunted_places['Haunted Places Date'] = haunted_places['description'].apply(extract_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2025-03-03\n",
       "1        2025-03-01\n",
       "2        2025-01-01\n",
       "3        0211-03-11\n",
       "4        2025-01-01\n",
       "            ...    \n",
       "10987    2025-03-12\n",
       "10988    2025-01-01\n",
       "10989    2025-03-18\n",
       "10990    2025-01-01\n",
       "10991    2025-01-01\n",
       "Name: Haunted Places Date, Length: 10992, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "haunted_places['Haunted Places Date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. d) Haunted Places Witness Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             description  \\\n",
      "0      Ada witch - Sometimes you can see a misty blue...   \n",
      "1      A little girl was killed suddenly while waitin...   \n",
      "2      If you take Gorman Rd. west towards Sand Creek...   \n",
      "3      In the 1970's, one room, room 211, in the old ...   \n",
      "4      Kappa Delta Sorority - The Kappa Delta Sororit...   \n",
      "...                                                  ...   \n",
      "10987  at 12 midnight you can see a lady with two lit...   \n",
      "10988  Is haunted by the victims of a murder that hap...   \n",
      "10989  The institution was for kids 18 years old and ...   \n",
      "10990  Gymnasium -  their have been reports of a litt...   \n",
      "10991  Cadets from the Air Force Academy participatin...   \n",
      "\n",
      "       Haunted Places Witness Count  \n",
      "0                                 0  \n",
      "1                                 0  \n",
      "2                                 0  \n",
      "3                                 2  \n",
      "4                                 0  \n",
      "...                             ...  \n",
      "10987                             2  \n",
      "10988                             0  \n",
      "10989                             0  \n",
      "10990                             0  \n",
      "10991                             0  \n",
      "\n",
      "[10992 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from number_parser import parse_number\n",
    "\n",
    "def preprocess_description(description):\n",
    "    \"\"\"Replace vague phrases with estimated numbers to aid extraction.\"\"\"\n",
    "    description = description.lower()\n",
    "    # Replace ambiguous phrases with approximate numbers\n",
    "    replacements = {\n",
    "        \"some\": \"3\",\n",
    "        \"a few\": \"3\",\n",
    "        \"several\": \"5\",\n",
    "        \"many\": \"10\",\n",
    "        \"a lot\": \"10\",\n",
    "        \"a handful\": \"5\",\n",
    "        \"numerous\": \"10\",\n",
    "        \"countless\": \"15\",\n",
    "        \"dozens\": \"12\",\n",
    "        \"scores\": \"20\",\n",
    "        \"hundreds\": \"100\",\n",
    "        \"a couple\": \"2\"\n",
    "    }\n",
    "    \n",
    "    for word, num in replacements.items():\n",
    "        description = re.sub(rf\"\\b{word}\\b\", num, description)  # Whole-word replacement\n",
    "    return description\n",
    "\n",
    "\n",
    "def extract_numbers_from_text(text):\n",
    "    \"\"\"Extract numerical values from written-out numbers in the text.\"\"\"\n",
    "    # Define a regex pattern to match written-out numbers\n",
    "    number_words = r'\\b(?:zero|one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|twenty|thirty|forty|fifty|sixty|seventy|eighty|ninety|hundred|thousand|million|billion)\\b'\n",
    "    \n",
    "    # Find all matches of written-out numbers\n",
    "    matches = re.findall(number_words, text.lower())\n",
    "    \n",
    "    # Parse each match into a numerical value\n",
    "    numbers = [parse_number(match) for match in matches if parse_number(match) is not None]\n",
    "    \n",
    "    # Filter out irrelevant numbers (e.g., years, small numbers)\n",
    "    filtered_numbers = [num for num in numbers if not (1900 <= num <= 2100)]  # Remove years\n",
    "    filtered_numbers = [num for num in filtered_numbers if num > 1]  # Ignore small numbers\n",
    "    \n",
    "    return filtered_numbers\n",
    "\n",
    "\n",
    "def extract_witness_count(description):\n",
    "    \"\"\"\n",
    "    Extract witness count from a haunted place description.\n",
    "    Returns a tuple (witness_count, method) where method indicates how the count was derived.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Preprocess the description\n",
    "        preprocessed_text = preprocess_description(description)\n",
    "        \n",
    "        # Step 2: Extract numbers from the text\n",
    "        numbers = extract_numbers_from_text(preprocessed_text)\n",
    "        \n",
    "        # Step 3: Return the first valid number found, or 0 if no numbers are found\n",
    "        if numbers:\n",
    "            return numbers[0], \"explicit_number\"\n",
    "        \n",
    "        # Step 4: Default to 0 if no numbers are found\n",
    "        return 0, \"default\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing witness count from description: {description[:100]}... Error: {e}\")\n",
    "        return 0, \"error\"\n",
    "\n",
    "\n",
    "# Apply the function to the 'description' column\n",
    "haunted_places['Haunted Places Witness Count'] = haunted_places['description'].apply(\n",
    "    lambda desc: extract_witness_count(desc)[0]  # Extract only the count (not the method)\n",
    ")\n",
    "\n",
    "# Display the updated columns\n",
    "print(haunted_places[['description', 'Haunted Places Witness Count']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. e) Time of Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_of_day(description):\n",
    "    time_keywords = {\"evening\": \"Evening\", \"morning\": \"Morning\", \"dusk\": \"Dusk\"}\n",
    "    for keyword, time_of_day in time_keywords.items():\n",
    "        if re.search(rf'\\b{keyword}\\b', description, re.IGNORECASE):\n",
    "            return time_of_day\n",
    "    return \"Unknown\"\n",
    "\n",
    "haunted_places['Time of Day'] = haunted_places['description'].apply(extract_time_of_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. f) Apparition Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_apparition_type(description):\n",
    "    apparition_keywords = {\n",
    "        \"ghost\": \"Ghost\",\n",
    "        \"orb\": \"Orb\",\n",
    "        \"ufo\": \"UFO\",\n",
    "        \"uap\": \"UAP\",\n",
    "        \"male\": \"Male\",\n",
    "        \"female\": \"Female\",\n",
    "        \"child\": \"Child\",\n",
    "        \"several ghosts\": \"Several Ghosts\"\n",
    "    }\n",
    "    for keyword, apparition_type in apparition_keywords.items():\n",
    "        if re.search(rf'\\b{keyword}\\b', description, re.IGNORECASE):\n",
    "            return apparition_type\n",
    "    return \"Unknown\"\n",
    "\n",
    "haunted_places['Apparition Type'] = haunted_places['description'].apply(extract_apparition_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. g) Event type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_event_type(description):\n",
    "    event_keywords = {\n",
    "        \"murder\": \"Murder\",\n",
    "        \"die\": \"Death\",\n",
    "        \"supernatural\": \"Supernatural Phenomenon\"\n",
    "    }\n",
    "    for keyword, event_type in event_keywords.items():\n",
    "        if re.search(rf'\\b{keyword}\\b', description, re.IGNORECASE):\n",
    "            return event_type\n",
    "    return \"Unknown\"\n",
    "\n",
    "haunted_places['Event Type'] = haunted_places['description'].apply(extract_event_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          city        country  \\\n",
      "1520     Lenox  United States   \n",
      "3394  Woodward  United States   \n",
      "8524     Paris  United States   \n",
      "\n",
      "                                            description  \\\n",
      "1520  An eleven year old girl, whose last name is Sl...   \n",
      "3394  This coffee house was originally a doctor's of...   \n",
      "8524  Most people get a bad feeling just looking at ...   \n",
      "\n",
      "                                 location          state state_abbrev  \\\n",
      "1520                     Cranewell Resort  Massachusetts           MA   \n",
      "3394                     Leos Coffeehouse       Oklahoma           OK   \n",
      "8524  Old Plantation home in Slate Shoals          Texas           TX   \n",
      "\n",
      "      longitude   latitude  city_longitude  city_latitude  Audio Evidence  \\\n",
      "1520 -73.267236  42.341822      -73.284876      42.356461           False   \n",
      "3394 -99.393019  36.434108      -99.390386      36.433648           False   \n",
      "8524        NaN        NaN      -95.555513      33.660939           False   \n",
      "\n",
      "      Image/Video/Visual Evidence Haunted Places Date  \\\n",
      "1520                        False          2025-01-01   \n",
      "3394                        False          2025-01-01   \n",
      "8524                        False          2025-05-11   \n",
      "\n",
      "      Haunted Places Witness Count Time of Day Apparition Type Event Type  \n",
      "1520                            11     Unknown         Unknown    Unknown  \n",
      "3394                            11     Unknown         Unknown    Unknown  \n",
      "8524                            11     Unknown         Unknown    Unknown  \n"
     ]
    }
   ],
   "source": [
    "#to check\n",
    "visual_evidence_records = haunted_places[haunted_places['Haunted Places Witness Count'] == 11]\n",
    "\n",
    "# Display the filtered records\n",
    "print(visual_evidence_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. h)  Merge the Alcohol Abuse Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data folder: /Users/yungyeechia/Desktop/DSCI550-HW1-main/Source Code/../Data\n",
      "Files in Data folder: ['counties.geojson', 'cleaned_crime_data.tsv', '.DS_Store', 'haunted_places.csv', 'NRIS_CR_Standards_Public.gdb.zip', 'haunted_places_with_alcohol.tsv', 'haunted_places_cleaned.csv', 'daylight_hours_full.tsv', 'haunted_places_with_alcohol_daylight.tsv', 'daylight_tidy.tsv', 'georef-united-states-of-america-county.geojson', 'haunted_places_cleaned_copy.csv', '2020_USRC_Summaries.xlsx', 'extracted_gdb', 'joined1.csv', 'haunted_places_with_sites.csv', 'cleaned_haunted_places.tsv', 'historic_sites.csv', 'alcohol_abuse.tsv']\n",
      "'/Users/yungyeechia/Desktop/DSCI550-HW1-main/Source Code/../Data/haunted_places.tsv' not found. Using 'cleaned_haunted_places.tsv' instead.\n",
      "Merge completed: 10974 rows merged. Merged file saved at: /Users/yungyeechia/Desktop/DSCI550-HW1-main/Source Code/../Data/haunted_places_with_alcohol.tsv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Determine the path to the Data folder relative to the current working directory\n",
    "data_folder = os.path.join(os.getcwd(), \"..\", \"Data\")\n",
    "print(\"Data folder:\", data_folder)\n",
    "print(\"Files in Data folder:\", os.listdir(data_folder))\n",
    "\n",
    "# Set the file path for the Haunted Places dataset:\n",
    "# First, try \"haunted_places.tsv\". If it doesn't exist, use \"cleaned_haunted_places.tsv\".\n",
    "haunted_file = os.path.join(data_folder, \"haunted_places.tsv\")\n",
    "if not os.path.exists(haunted_file):\n",
    "    print(f\"'{haunted_file}' not found. Using 'cleaned_haunted_places.tsv' instead.\")\n",
    "    haunted_file = os.path.join(data_folder, \"cleaned_haunted_places.tsv\")\n",
    "\n",
    "# Set the file path for the Alcohol Abuse dataset\n",
    "alcohol_file = os.path.join(data_folder, \"alcohol_abuse.tsv\")\n",
    "\n",
    "# Load Haunted Places dataset\n",
    "haunted_df = pd.read_csv(haunted_file, sep=\"\\t\")\n",
    "haunted_df.columns = [col.strip().lower() for col in haunted_df.columns]\n",
    "\n",
    "# Load Alcohol Abuse dataset (assuming it's comma-separated)\n",
    "alcohol_df = pd.read_csv(alcohol_file, sep=\",\")\n",
    "alcohol_df.columns = [col.strip().lower() for col in alcohol_df.columns]\n",
    "\n",
    "# Rename the Alcohol Abuse column if necessary to ensure a common key \"state\"\n",
    "if \"state\" not in alcohol_df.columns and \"state_name\" in alcohol_df.columns:\n",
    "    alcohol_df.rename(columns={\"state_name\": \"state\"}, inplace=True)\n",
    "\n",
    "# Check that both DataFrames have the 'state' column\n",
    "if \"state\" not in haunted_df.columns or \"state\" not in alcohol_df.columns:\n",
    "    raise KeyError(\"The 'state' column is missing from one of the datasets.\")\n",
    "\n",
    "# Merge the datasets on the 'state' column using a left join\n",
    "merged_df = pd.merge(haunted_df, alcohol_df, on=\"state\", how=\"left\")\n",
    "\n",
    "# Save the merged dataset as a TSV file in the Data folder\n",
    "merged_file = os.path.join(data_folder, \"haunted_places_with_alcohol.tsv\")\n",
    "merged_df.to_csv(merged_file, sep=\"\\t\", index=False)\n",
    "print(\"Merge completed: {} rows merged. Merged file saved at: {}\".format(merged_df.shape[0], merged_file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. i)  Merge the Alcohol Abuse Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting extraction from Time and Date website...\n",
      "Page source obtained.\n",
      "Browser closed.\n",
      "Number of tables found on the page: 3\n",
      "Table 1 extracted with shape: (7, 2)\n",
      "Table 2 extracted with shape: (27, 9)\n",
      "Table 3 extracted with shape: (199, 5)\n",
      "Data folder exists at: /Users/yungyeechia/Desktop/DSCI550-HW1-main/Data\n",
      "Scraped data saved to: /Users/yungyeechia/Desktop/DSCI550-HW1-main/Data/daylight_hours_full.tsv\n",
      "File successfully saved!\n",
      "Extraction complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "def extract_timeanddate_daylight(output_filename=\"daylight_hours_full.tsv\"):\n",
    "    print(\"Starting extraction from Time and Date website...\")\n",
    "    # Set up Selenium in headless mode\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(options=options)  # Ensure chromedriver is in your PATH\n",
    "\n",
    "    try:\n",
    "        # Navigate to the Time and Date Astronomy page for the USA\n",
    "        url = \"https://www.timeanddate.com/astronomy/usa\"\n",
    "        driver.get(url)\n",
    "        driver.implicitly_wait(10)  # Wait up to 10 seconds for dynamic content\n",
    "        # Get the page source\n",
    "        html = driver.page_source\n",
    "        print(\"Page source obtained.\")\n",
    "    finally:\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "        print(\"Browser closed.\")\n",
    "\n",
    "    # Parse the HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Find all <table> elements on the page\n",
    "    tables = soup.find_all(\"table\")\n",
    "    print(\"Number of tables found on the page:\", len(tables))\n",
    "\n",
    "    # Convert each table to a Pandas DataFrame and collect them\n",
    "    dfs = []\n",
    "    for i, table in enumerate(tables, start=1):\n",
    "        try:\n",
    "            df = pd.read_html(str(table))[0]\n",
    "            dfs.append(df)\n",
    "            print(\"Table\", i, \"extracted with shape:\", df.shape)\n",
    "        except ValueError as e:\n",
    "            print(\"Skipping a table (index\", i, \") due to error:\", e)\n",
    "\n",
    "    if not dfs:\n",
    "        print(\"No valid tables found to scrape. Exiting.\")\n",
    "        return None\n",
    "\n",
    "    if len(dfs) > 1:\n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    else:\n",
    "        combined_df = dfs[0]\n",
    "\n",
    "    # Define the path to the Data folder (one level up)\n",
    "    data_folder = os.path.join(os.getcwd(), \"..\", \"Data\")\n",
    "    if not os.path.exists(data_folder):\n",
    "        os.makedirs(data_folder)\n",
    "        print(\"Data folder created at:\", os.path.abspath(data_folder))\n",
    "    else:\n",
    "        print(\"Data folder exists at:\", os.path.abspath(data_folder))\n",
    "    \n",
    "    # Save the combined DataFrame as a TSV file in the Data folder\n",
    "    output_filename = os.path.join(data_folder, output_filename)\n",
    "    combined_df.to_csv(output_filename, sep=\"\\t\", index=False)\n",
    "    full_path = os.path.abspath(output_filename)\n",
    "    \n",
    "    if os.path.exists(full_path):\n",
    "        print(\"Scraped data saved to:\", full_path)\n",
    "        print(\"File successfully saved!\")\n",
    "    else:\n",
    "        print(\"Error: File was not saved to\", full_path)\n",
    "    return full_path\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_timeanddate_daylight()\n",
    "    print(\"Extraction complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file from: ../Data/daylight_hours_full.tsv\n",
      "After dropping metadata rows, number of rows: 228\n",
      "Tidy DataFrame (first 10 rows):\n",
      "           location    sunrise     sunset\n",
      "0         Adak (AK)  ↑ 9:08 am  ↓ 8:45 pm\n",
      "1       Albany (NY)  ↑ 7:13 am  ↓ 6:57 pm\n",
      "2  Albuquerque (NM)  ↑ 7:22 am  ↓ 7:11 pm\n",
      "3         Ames (IA)  ↑ 7:32 am  ↓ 7:16 pm\n",
      "4    Anchorage (AK)  ↑ 8:27 am  ↓ 7:52 pm\n",
      "5    Annapolis (MD)  ↑ 7:23 am  ↓ 7:09 pm\n",
      "6      Atlanta (GA)  ↑ 7:52 am  ↓ 7:42 pm\n",
      "7      Augusta (ME)  ↑ 6:58 am  ↓ 6:40 pm\n",
      "8       Austin (TX)  ↑ 7:45 am  ↓ 7:36 pm\n",
      "9    Baltimore (MD)  ↑ 7:23 am  ↓ 7:09 pm\n",
      "Final cleaned data with daylight hours (first 10 rows):\n",
      "           location  sunrise   sunset state  daylight_hours\n",
      "0         Adak (AK)  9:08 am  8:45 pm    AK           11.62\n",
      "1       Albany (NY)  7:13 am  6:57 pm    NY           11.73\n",
      "2  Albuquerque (NM)  7:22 am  7:11 pm    NM           11.82\n",
      "3         Ames (IA)  7:32 am  7:16 pm    IA           11.73\n",
      "4    Anchorage (AK)  8:27 am  7:52 pm    AK           11.42\n",
      "5    Annapolis (MD)  7:23 am  7:09 pm    MD           11.77\n",
      "6      Atlanta (GA)  7:52 am  7:42 pm    GA           11.83\n",
      "7      Augusta (ME)  6:58 am  6:40 pm    ME           11.70\n",
      "8       Austin (TX)  7:45 am  7:36 pm    TX           11.85\n",
      "9    Baltimore (MD)  7:23 am  7:09 pm    MD           11.77\n",
      "Tidy daylight data saved as: ../Data/daylight_tidy.tsv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Locating the data folder and file paths using relative paths\n",
    "data_folder = \"../Data\"  # Moves up one level to access \"Data/\"\n",
    "raw_file = os.path.join(data_folder, \"daylight_hours_full.tsv\")\n",
    "output_file = os.path.join(data_folder, \"daylight_tidy.tsv\")\n",
    "\n",
    "# Check if the file exists before proceeding\n",
    "if not os.path.exists(raw_file):\n",
    "    raise FileNotFoundError(f\"File not found: {raw_file}\")\n",
    "\n",
    "print(\"Reading file from:\", raw_file)\n",
    "\n",
    "# Load the Raw Data\n",
    "df = pd.read_csv(raw_file, sep=\"\\t\", dtype=str)\n",
    "\n",
    "# Drop metadata rows if the first row contains \"Country:\"\n",
    "if df.iloc[0, 0].strip().lower() == \"country:\":\n",
    "    df = df.iloc[5:].reset_index(drop=True)\n",
    "\n",
    "print(\"After dropping metadata rows, number of rows:\", df.shape[0])\n",
    "\n",
    "# Extract relevant blocks (Location, Sunrise, Sunset)\n",
    "# Identify relevant columns manually\n",
    "if df.shape[1] >= 11:\n",
    "    blocks = [\n",
    "        df.iloc[:, [2, 3, 4]].copy(),\n",
    "        df.iloc[:, [5, 6, 7]].copy(),\n",
    "        df.iloc[:, [8, 9, 10]].copy(),\n",
    "    ]\n",
    "else:\n",
    "    raise ValueError(\"Not enough columns in the dataset. Check the table structure.\")\n",
    "\n",
    "# Rename columns consistently\n",
    "for block in blocks:\n",
    "    block.columns = [\"location\", \"sunrise\", \"sunset\"]\n",
    "\n",
    "# Concatenate all blocks into one dataframe\n",
    "tidy_df = pd.concat(blocks, ignore_index=True)\n",
    "\n",
    "# Drop empty rows\n",
    "tidy_df = tidy_df[tidy_df[\"location\"].notna() & (tidy_df[\"location\"].str.strip() != \"\")]\n",
    "tidy_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"Tidy DataFrame (first 10 rows):\")\n",
    "print(tidy_df.head(10))\n",
    "\n",
    "# Extracting state abbreviations from location\n",
    "# Assumes locations have a format like \"Philadelphia (PA)\"\n",
    "tidy_df[\"state\"] = tidy_df[\"location\"].apply(\n",
    "    lambda x: re.search(r\"\\(([A-Z]{2})\\)\", x).group(1) if re.search(r\"\\(([A-Z]{2})\\)\", x) else None\n",
    ")\n",
    "\n",
    "# Clean up sunrise and sunset times\n",
    "# Remove any symbols (e.g., \"↑\", \"↓\") and strip whitespace\n",
    "tidy_df[\"sunrise\"] = tidy_df[\"sunrise\"].str.replace(\"↑\", \"\", regex=False).str.strip()\n",
    "tidy_df[\"sunset\"] = tidy_df[\"sunset\"].str.replace(\"↓\", \"\", regex=False).str.strip()\n",
    "\n",
    "# Function to calculate daylight hours\n",
    "def calculate_daylight_hours(row):\n",
    "    try:\n",
    "        sunrise_time = datetime.strptime(row[\"sunrise\"], \"%I:%M %p\")\n",
    "        sunset_time = datetime.strptime(row[\"sunset\"], \"%I:%M %p\")\n",
    "        daylight_hours = (sunset_time - sunrise_time).seconds / 3600  # Convert to hours\n",
    "        return round(daylight_hours, 2)  # Round to 2 decimal places\n",
    "    except Exception as e:\n",
    "        return None  # Return None if there's an issue\n",
    "\n",
    "# Apply function to calculate daylight hours\n",
    "tidy_df[\"daylight_hours\"] = tidy_df.apply(calculate_daylight_hours, axis=1)\n",
    "\n",
    "print(\"Final cleaned data with daylight hours (first 10 rows):\")\n",
    "print(tidy_df.head(10))\n",
    "\n",
    "# Save the cleaned data using a relative path\n",
    "tidy_df.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "print(\"Tidy daylight data saved as:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset saved to: ../Data/merged_haunted_with_alcohol_daylight.tsv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define relative file paths\n",
    "daylight_file = \"../Data/daylight_tidy.tsv\"  # Corrected file path\n",
    "haunted_file = \"../Data/haunted_places_with_alcohol.tsv\"\n",
    "output_file = \"../Data/merged_haunted_with_alcohol_daylight.tsv\"\n",
    "\n",
    "# Check if files exist before proceeding\n",
    "for file_path in [daylight_file, haunted_file]:\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Error: File not found at {file_path}. Check your Data/ folder.\")\n",
    "\n",
    "# Load the datasets\n",
    "df_daylight = pd.read_csv(daylight_file, sep=\"\\t\")\n",
    "df_haunted = pd.read_csv(haunted_file, sep=\"\\t\")\n",
    "\n",
    "# Merge the datasets on state abbreviations\n",
    "merged_df = df_haunted.merge(df_daylight, left_on=\"state_abbrev\", right_on=\"state\", how=\"left\")\n",
    "\n",
    "# Drop duplicate state column\n",
    "merged_df.drop(columns=[\"state_y\"], inplace=True)\n",
    "\n",
    "# Rename columns for clarity\n",
    "merged_df.rename(columns={\"state_x\": \"state\"}, inplace=True)\n",
    "\n",
    "# Save the merged dataset using a relative path\n",
    "merged_df.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "\n",
    "print(f\"Merged dataset saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
